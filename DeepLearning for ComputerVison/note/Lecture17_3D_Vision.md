# 三维视觉概述

前面我们我们讨论了视觉中的不同任务，包括语义分割，目标检测，实例分割等等，我们还讨论了关键点估计的一些话题，这些算法在实际生活中有非常多的应用，当然，这些算法都是基于二维图像的

但是，我们的世界不是二维的，而是三维的，这是一个完全独立的研究领域，研究如何增加空间维度到我们的神经网络模型，所以今天的内容就是如何将三维信息传递到神经网络模型中

![4](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_5.jpg?)

## 两类问题

我们这里要关注两类问题

第一个就是从单个图像预测三维形状的任务（如下图所示），我们输入一些彩色图像，然后输出该图像中对象的三维形状的表示

第二个就是形状预测的任务，输入一些三维形状数据，然后进行预测，比如说分类或者分割任务

![5](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_6.jpg?)

这些任务都是基于监督学习的，所以我们有一个带有输入图像和对象三维形状的训练集（或者三维形状和对应标签的）

## 三维视觉中的更多主题

三维视觉的内容不仅仅是完成分类分割，实际上还有更多的相关主题（很多主题并不是基于深度学习方法的），这是因为三维视觉会涉及对象和世界的三维结构

![6](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_7.jpg?)

我们可能有这样一直想法，就是我们想输入一个视频序列，这是一个二维图像帧序列，然后我们想通过视频，预测或者重建摄像机穿过三维世界的轨迹

但是在这里我们只涉及两部分，一个是有监督形状预测，一个是有监督形状分类

# 三维形状表示（3D Shape Representations）

首先，我们应该如何去表述三维形状呢？我们这里有五种表示方式，可以用来模拟三维形状和信息，这也是人们常用的类型，有不同的优缺点，每一种都可以用神经网络模型去预测和处理

下图中展示了五种表达方式，每一种都是相同底层三维形状的不同表示

![7](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_8.jpg?)

## 深度图（Depth Map）

### 概念

深度图在概念上是一个非常简单的三维形状表示，深度图的作用是对于输入图像中的每个像素，分配从相机到该像素的距离，因为图像中的每个像素都对应于现实世界中的某个对象，现在深度图告诉我们，对于图像中的每个像素，相机与该像素试图表示的现实世界中的那个位置之间的距离是多少，单位是米。

![9](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_10.jpg?)

不过深度图比以往的RGB图要多一个网格，其中每个像素值给了我们一个深度信息，有些类似于RGB图像的二维网格，只不过像素的值不是颜色，而是“深度”，以米为单位的深度，然后将其与RGB图像结合，深度作为第四个信息通道，提供深度信息

或者我们可以叫他为2.5D图像或者RGB-D图像，因为深度图不像一个真正的完整三维表示，因为它的缺点是不能正确捕捉被遮挡物体的结构，比如说上图中，书柜的一部分被沙发遮挡了，那么相应的深度图就没有沙发后面书柜的任何三维表示，所以说RGB-D只能表示图像的可见部分，出于这个原因，我们可能认为它是一种不是很强大或者不是很通用的三维表示

但是这种深度图类型的数据是很重要的，因为我们可以使用各种原始3D传感器去捕获深度图数据，比如说结构光相机

### 深度预测

不过我们可以想尝试一个任务，就是输入一个普通RGB图像，然后尝试逐像素地预测相机到对应物体的距离（如下图所示），使用全卷积网络完成这个深度预测任务

![10](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_11.jpg?)

但是实际上这是不太可行的，因为在三维视觉中，会有**尺度/深度歧义（Scale / Depth Ambiguity）**的问题，比如说对于一张图片，无法区分出远处大物体和近处小物体，或者说仅通过单张二维图像，我们无法准确地确定物体的实际大小和距离

如下图所示，大猫是小猫的两倍大小，但是与相机的距离也是小猫的两倍，那么看起来是完全一样的大小

![11](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_12.jpg?)

也就是说，绝对尺度和绝对深度在单个二维图像中并不明确，所以会产生这种问题，所以在处理任何类型的三维表示或者三维预测问题的时候，考虑这个潜在的问题是很有必要的

但是我们可以通过改变神经网络的结构来处理尺度/深度歧义问题，或者说我们可以通过改进损失函数来完成深度预测问题

![12](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_13.jpg?)

因为尺度和深度实际上是不变的，假设我们的网络模型预测了一个深度，达到了Ground Truth Depth的某个恒定比例，那么上图中的尺度不变损失函数仍会将这个情况认定为零损失，或者说，这个尺度不变损失函数只会关心预测深度是不是匹配真实深度的某个固定倍数，这个属性细节叫做比例方差

### 表面法线（Surface Normals）

这是第二种三维形状表示，想法上与RGB-D深度图很接近，只不过在这里，我们给每个像素分配的是一个单位向量（或者说法线）

每个表面点的法线都是垂直于该表面的单位向量。对于平面，法线的方向是确定的，但对于曲面，每个点的法线可能都不同。在计算机图形学和计算机视觉中，表面法线被广泛用于光照计算，以判断物体表面的亮度和颜色。

理解表面法线也对物体识别和场景理解有重要作用。例如，通过分析一幅图像中的表面法线，我们可以推断出物体的形状和结构。在一些深度学习应用中，表面法线也被用作一种重要的特征来帮助模型更好地理解场景。

![13](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_14.jpg?)

不过在这里，我们会使用RGB颜色来绘制法线的情况，比如说上图中，蓝紫色表示法向量是向上的，红色表示向前的

当然我们也可以使用一个网络来预测表面法线，我们可以输入RGB图像，然后去预测每个位置的三维向量

![14](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_15.jpg?)

然后我们的损失函数是对比两个法向量之间角度的差异，这里我们使用点积除以范数的方式进行归一化，以此来训练网络

或者可以训练一个联合网络，同时完成语义分割、深度估计和表面法线估计

这是一个相对简单的表示，但是在实践中是有用的，因为一旦有了深度图和表面法线，就可以得到很多关于图像三维结构的信息

当然，缺点还是不能表示被遮挡的地方

## 体素网格（Voxel Grid）

### 概念

"Voxel"是"volume"（体积）和"pixel"（像素）的结合词，指的是在三维空间中的像素，也就是体素

体素网格就像是三维的像素网格，它将三维空间划分成了一系列的小立方体（也就是体素），可以使用某些方法看到每个体素是否被占用（比如一个bool值），这有点像我的世界一样。每个体素都可以存储一些信息，例如颜色、密度、表面法线等，有点像Mask R-CNN中物体背景和前景的表示。通过这种方式，我们可以在计算机中表示和处理三维对象。

![16](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_17.jpg?)

体素网格在许多领域都有应用，如计算机图形学、医学成像、地质科学和机器人导航等。例如，在计算机图形学中，体素网格可以用于渲染复杂的三维场景；在医学成像中，MRI和CT扫描的结果通常就是体素网格；在机器人导航中，体素网格可以用于表示环境并进行路径规划。

然而，体素网格也有一些缺点，例如它需要大量的存储空间，而且处理速度可能比较慢，尤其是在需要捕捉非常精细的细节的时候，分辨率回非常高，比如说图中的椅子就使用了非常多的像素。因此，在实际应用中，人们经常使用各种优化技术，如八叉树（Octree）和稀疏哈希表（Sparse Hashing），来高效地存储和处理体素网格。

### 处理：三维卷积

如果我们想对这些体素网格进行分类的话，一样可以使用卷积网络的架构，不过我们需要使用一种三维卷积的方式去完成

输入数据是一个原始的体素网格，每个点都有占用或者未占用两种情况，如何我们使用三维卷积核（类似于一个立方体）去滑动来计算内积，有点类似于一个立方体滑动着遍历这个空间的每个地方，然后产生下一层的标量输出，若干次卷积处理之后进入分类层，这在架构上与之前的二维卷积完全一致

![17](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_18.jpg?)

当然，这里维度还是稍微有所不同的，每个阶段的张量都是一个四维张量，三个空间维度和一个特征维度（或者通道维度，表示体素是否被占用，这个是二进制的）

### 预测体素：三维卷积

我们下一个想完成的任务就是使用RGB图像去预测三维形状的体素网格

左边输入的是一个RGB图像，两个空间维度和一个RGB通道维度，我们需要预测一个四维张量，带有三个通道维度和一个通道维度，可以通过网格中每个点的占用概率信息，这需要我们设置一些架构，来添加额外的空间维度，然后使用交叉熵损失来训练这个网络

![18](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_19.jpg?)

一种常见的完成这个任务的方式就是使用桥接的方式去连接不同维度的张量

首先我们对这个RGB图像使用卷积网络处理得到二维图像的特征图（这是一个三维张量，空间维度加一个特征维度），然后展开成一个特征向量，然后使用一个全连接层将其重塑为四维张量，然后我们就可以使用上采样方式得到三维表示

当然，这种方式的计算成本很大，所以并不适合

### 预测体素：体素管

"Voxel Tubes"是一种从二维图像生成三维体素网格的方法。这个方法基于一个假设，即每个像素的颜色和亮度都对应于一个小的、垂直于图像平面的体素管（Voxel Tube），这些体素管可以被组合在一起，形成一个完整的三维体素网格。这种方法只基于二维卷积，在计算上更有用

首先我们输入一张RGB图像，然后使用二维卷积得到一个CxHxW的特征图，但是在网络的最后一层非常特殊，因为我们想预测体素的输出，那么就要在这里安排空间卷积

不过，在这里实际上使用两个空间维度和一个通道维度，但是在计算损失的时候，我们会解释通道维度为输出张量的深度维度

在卷积的最后一层，就沿着通道维度去预测一个Tube，给我们预测一整管的体素概率

![19](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_20.jpg?)

在这个方法中，每个体素管的高度（也就是在深度方向上的大小）可以由图像的亮度决定，而宽度和长度（在图像平面上的大小）可以由像素的大小决定。这样，我们就可以从一个二维图像生成一个粗糙的三维体素网格。

不过问题是，当我们使用三维卷积核去卷积的时候，是牺牲了Z方向的平移不变性的，但是在XY方向上仍然有很好的平移不变性

然而，这个方法有一些限制。首先，它假设图像中的每个像素都对应于一个体素管，这在许多情况下可能并不成立。例如，对于遮挡和反射等现象，这个方法可能无法处理。其次，这个方法生成的体素网格可能比较粗糙，不能精确地表示物体的形状。

为了解决这些问题，我们可以使用深度学习的方法，如卷积神经网络和生成对抗网络（GAN），来改进这个过程。这些方法可以学习从二维图像到三维体素网格的映射关系，生成更精确和细致的体素网格。

### 问题

体素表示的问题就是占用空间很多，比如说我们使用32位浮点数来表示体素，那么一个1020x1024x1024大小的体素，就需要使用4GB的空间

![20](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_21.jpg?)

当然也是有解决方法的

### 多分辨率体素：八叉树

这种体素的想法是一个体素网格可以有多重分辨率，这样可以使用一些低分辨率体素去构建主要结构，然后使用高分辨率的体素去构建细节，高分辨率体素可能是低分辨率体素的稀疏子集

![21](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_22.jpg?)

### 多分辨率体素：嵌套形状层

这种方法的思想是，与其表示为密集的体素网格，不如表示为完整三维形状，或者说将对象形状从内到外的表示出来

我们有一些粗糙的外层，然后有一些负体素，我们可以稀疏的表示

我们可以将一个物体表示为不同的稀疏体素层的总和

![22](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_23.jpg?)

## 隐函数法（Implicit Functions）

这是一种隐式表面的想法：我们想将三维形状表示为一个函数，所以我们需要做的就是学习一些函数，输入一些三维空间坐标，就可以输出任何位置被占据的概率

与体素网格这种方法不同，体素网格这种方法就是在空间中一些有限点集上对这样的函数进行采样，然后将这些样本以某种显式网格表示形式存储到函数中，但是我们在这里使用隐式函数，使用这种数学函数本身来隐式地表示这些三维形状，然后我们可以在3D空间中的任意放置点从这个函数中采样，它应该告诉我们这个位置是在对象内部还是外部

![25](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_26.jpg?)

在实践中，通常把对象表示为层次分明的点集，比如说物体表面的占用概率就是0.5，我们可以直观的的显示，如上图所示，蓝色代表非常接近1的值，红色代表非常接近0的值，白色区域是0.5水平聚集的地方，代表三维形状的实际表面，或者可以成为带符合的距离函数

其思想是这种函数给出了三维空间中的点到表面的欧几里得距离，距离的正负决定了点在对象内外

当然这个函数可能会非常的复杂，所以我们会使用神经网络去学习这个函数表示

![28](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_29.jpg?)

## 点云（Point Cloud）

### 概念

点云，实际上就是将物体表示为三维空间中的点集（是一组没有顺序的点，顺序不影响点云本身），或者说使用一组点集以某种方式覆盖我们想表示的三维形状的表面，比如说下图中的飞机，点云本身有丰富的空间信息，但是一般来说缺少图像那样丰富的语义信息

与体素网格相比，点云某种程度上更有适应性，如果我们想使用体素网格表示精细的细节，那么分辨率就会很高，但是在点云中，我们可以使用不同的点云密度来改变细节的精细程度

当然，点云也有缺点，如果你想实际提取一些三维形状来可视化，那么基需要某种后处理，因为在数学上，点云中的点是无限小的一个质点，但是在可视化的时候我们必须让每个点变成一个有限大小的球然后进行渲染，才可以在屏幕上可视化显示，同时，点云具有稀疏性

![31](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_32.jpg?)

但是点云在神经网络中是一个非常有用的东西，同时也是非常常见的，比如说在自动驾驶中，激光雷达就是收集了周围环境的点云表示，所以在自动驾驶程序中，点云是经常使用的，我们可以根据这些原始点云输入去做出一些决策

总的来说，点云是一种很好的表达形式，接近传感器原始数据，同时其表示起来很容易（一个多维向量就可以表示，坐标、颜色、强度等特征），同时可以更好的表示三维形状，在诸多领域都有很好的应用

![3](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/PointCloudDL_ShenlanOpen_LiuYongcheng_4.png)

但是注意一下，点云是一种无序的数据，不同的表示顺序对于点云并没有任何影响，并且点云还有平移/旋转不变性

### 数据集

在点云处理领域，有一些比较常用的数据集

1. ModelNet：每个物体点云由一千个点组成，而且是使用CAD模型直接生成的点云，所以其中是比较规范的没有噪声的点，常用于分类任务
2. ShapeNet：每个物体点云由两千个点组成，常用来做分割任务
3. PartNet：划分更精细，一个物体点云中包括了不同部件的划分，并且相对新，对机器人的抓取等任务更有用

![5](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/PointCloudDL_ShenlanOpen_LiuYongcheng_6.png)

上面的是一些相对独立的物体，下面是一些更偏实际大场景的数据集

4. Stanford 3D indoor scene：有八千个点
5. Semantic 3D：更大的数据集，更密集的点云
6. ScanNet：也是很大，可以做检测和分割
7. KITTI，Apollo，nuScenes，Waymo：偏向于工业实用

### 生成点云输出

我们想做的另一件事就是使用RGB图像去生成三维形状的点云输出，我们同样可以使用神经网络来完成这个操作

![33](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_34.jpg?)

### 损失函数

我们总是需要一种损失函数去训练网络，来比较生预测点云和实际标签之间的区别，或者说去比较二者之间的区别，同时需要这个函数是可微的，便于我们反向传播，这个函数就是钝化距离（Chamfer distance）

![38](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_39.jpg?)

我们想输入两组点，橙色和蓝色，然后钝角距离就会告诉我们，这两组点有何不同，实际上这个函数有两部分

第一部分，是对每个蓝色点，寻找最近的橙色点，然后我们将计算二者之间的欧氏距离，然后对所有的距离求和

第二项是一样的内容，不过是对橙色点而言的计算

两项最近邻匹配距离项的总和就是钝角函数，唯一使得损失函数为0的方法就是两个点云完全重合

它有几个特性使得它非常适合用于点云神经网络。

1. **无序性**：点云数据本身是无序的，点的顺序并不影响点云表示的实际物体形状。Chamfer distance 计算的是每个点到另一个点集中最近点的距离，这个计算过程与点的顺序无关，这是一种最近邻操作，这意味着，无论我们如何改变点云中点的顺序，Chamfer distance 的值都不会变，这与点云数据的无序性相符合。
2. **鲁棒性**：Chamfer distance 是一个双向的度量，它既考虑了第一个点集中每个点到第二个点集中最近点的距离，也考虑了第二个点集中每个点到第一个点集中最近点的距离。这意味着，如果一个点集中有一些点在另一个点集中没有对应的近邻点，Chamfer distance 也不会变得非常大。这使得它对噪声和离群点有很好的鲁棒性。
3. **简单易计算**：虽然 Chamfer distance 的直接计算需要对每一对点都计算距离，但是我们可以利用空间数据结构（如KD-tree）或者近似算法（如最近邻搜索）来加速计算，使得它在实践中可以高效地用于大规模的点云数据。

## 三角网格（Triangle Mesh）

### 概念

这是计算机图形学中非常常用的表示，有点类似于点云，因为它是将三维形状表示为三维空间中的一组顶点，在三角网格表示中，三维物体的表面被划分为许多小的三角形面片，这些三角形面片的集合就构成了三角网格。

![41](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_42.jpg?)

每个三角形面片由三个顶点（vertices）和三条边（edges）组成。顶点存储了空间中的坐标位置信息，可能还会包含其他属性信息，如颜色、法线、纹理坐标等。边连接了两个顶点，描述了三角形的形状。除此之外，三角网格还包括了面（faces），即由三条边围成的区域，它们描述了物体的表面。

三角网格的优点在于它可以准确和高效地表示复杂的三维表面（在计算机图形学中常用）。由于三角形是平面的，任何复杂的三维表面都可以通过足够数量的三角形面片来逼近。此外，三角网格还支持多种几何运算，如交、并、差等。

![44](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_45.jpg?)

### 预测三角网格：Pixel2Mesh

当然这也需要一种新网络去处理这种数据，比如说ECCV2018年的一篇新论文，就是一种使用神经网络处理网格的算法，称为Pixel2Mesh，这种网络，输入一个RGB图像，然后输出一个三角网格，给出一个图像中物体的完整三维形状

![46](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_47.jpg?)

当然其中有几个关键的地方

### 迭代精细化

第一个地方就是，我们想构建一个神经网络来输出三角网格，但是实际上很难从头创建一个网格对象，所以我们需要将一些初始网格模板输入到网络中，然后随着网络的学习，这个网格开始变形，最终变为我们所需的输出，所以我们需要将初始的球状或者椭圆体网格作为模板在初始阶段输入网络

![47](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_48.jpg?)

我们会以某种方式去查看初始网格与图像的匹配度，然后不断的对网格进行更新（或者说迭代精细化），最后就可以输出与输入图像的几何形状非常匹配的三角网格

### 图卷积神经网络

第二个地方就是我们需要一种在网格结构数据上可以操作的神经网络，这是因为网格并不是我们常见的规则化数据，在这里，我们使用的就是一种名为图卷积的运算

我们熟悉二维/三维卷积运算，我们通过卷积运算输出一个新的特征图，特征图中的特征取决于输入数据中某个局部感受野或者特征的局部淋雨，然后我们通过卷积核的不断滑动来计算所有的特征输出

但是，我们现在使用的数据，不是空间网格，而是任意图结构，所以我们这里的操作要进行相应的改变，图卷积层的输入是一个图和一个附加到图的每个顶点的特征向量

我们想计算图卷积层的输出的时候，依然延续感受野的概念，每个顶点的新特征向量和输出特征取决于输入图中特征向量到图卷积层的局部感受野，所以我们需要依赖于一种特色的数学形式来计算，输出特征向量$f^\prime_i$取决于输入特征向量和所有相邻顶点的特征向量

![48](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_49.jpg?)

然后我们将这个卷积函数应用与图中的每个顶点，这类似于图像卷积，这样就可以应用与任意数量顶点的图

我们认为这又是一种非常好的处理网格结构数据的方法，并提出了某种神经网络结构

![50](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_51.jpg?)

### 对齐

回顾任务，我们想根据一个RGB图像预测三角网格，所以我们需要一些方法将图像信息混合到图卷积网络中，所以我们有了一种对齐的想法

对于网格中每个顶点，我们想从图像中获得某种特征向量，代表图像在该顶点的空间位置上的视觉外观，我们可以完成的是，使用二维卷积网络处理图像来提取特征

![51](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_52.jpg?)

如果我们了解相机的内在特性，我们可以使用某种投影算子将我们的三维三角网格的顶点投影到图像平面上，对于每个顶点投影位置，我们使用双线性插值来从卷积网络输出的特征中进行采样，这样就可以使得每个顶点的特征向量完全对齐到图像平面中该特征的位置，这与我们之前在Mask R-CNN中的RoI对齐运算符的想法是一致的

![52](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_53.jpg?)

但是我们仍然想对特征向量和二维图像平面中的任意位置进行采样，而不是像RoI对齐哪有在规则网格中进行采样，我们现在要做的是在图像平面的每一点上对所有投影顶点位置的特征向量进行采样，这样才可以将图像信息混合到我们的图形卷积网络中

### 损失函数

最后一件事就是损失函数，但是，同一个形状有多重不同的表示，比如说下图中的一个正方形，可以使用两个或者四个三角形来表示，我们希望我们的损失函数不受我们用三角形表示形状的特定方式的影响，而是取决于基础形状本身，不是取决于表示方式

我们实际上是有方法的，就是沿着网格内部取样，将其转化为点云（如下图所示），然后使用钝化距离来比较两个点云之间的区别

![55](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_56.jpg?)

当然这里区分一下在线采样和离线采样的区别，在线采样是对预测的网格采样，离线采样是对标签网格采样

![59](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_60.jpg?)

当然，我们需要通过左边的采样操作进行反向传播，这又是一个复杂问题

# 形状比较指标

## 概念

我们也需要一些指标，来对比模型效果，便于查看模型是否训练良好，在二维目标检测中，我们会使用交并比来查看，在三维中，我们一样也可以使用类似的方法

![66](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_67.jpg?)

当然，在三维中，交并比可能并没有那么大的意义，所以我们可以使用钝化距离来比较我们的精度，这是因为钝化距离使用了L2距离，对异常值很敏感，很适合用来完成评价

![69](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_70.jpg?)

## F1分数

F1分数（F1 Score）是一种用于评估分类模型性能的统计指标，尤其在数据不平衡的情况下特别有用。它是精确率（Precision）和召回率（Recall）的调和平均值。

F1分数在点云上进行评估，有点类似于钝化距离，我们有两组点云（下图中橙色蓝色的两组），橙色为预测点云，蓝色为真实点云，如果预测点云在某个阈值半径内，那么就被认为是正确的

我们想象一下，每个预测点云周围扩展出一个球体，如果一些真实点云落在李沐，那么这个预测点云基被认为是真实的，在下图例子里面，四个橙色点云中三个是正确的，所以精度是3/4，三个真实点云只有两个被预测正确，那么召回率就是2/3，进行一个几何平均值处理就是F1分数

![73](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_74.jpg?)

- 精确率：在所有预测为正例的样本中，真正为正例的比例。
- 召回率：在所有真正为正例的样本中，被正确预测为正例的比例。

F1分数的公式为：

F1 = 2 * (精确率 * 召回率) / (精确率 + 召回率)

F1分数的值范围在0到1之间。1表示模型的表现最佳，0表示模型的表现最差。由于F1分数同时考虑了精确率和召回率，因此它能够更全面地评估模型的性能，并且对异常值更稳健，尤其在正负样本不平衡的情况下。

然而，F1分数并非在所有情况下都是最佳的评估指标。如果你更关心精确率或召回率，可能需要直接使用这两个指标，或者使用其他权衡两者的方法，如ROC曲线下的面积（AUC-ROC）。

# 常用3D传感器

3D常用的传感器可分为两类，**被动传感器 (Passive Sensors)**和**主动传感器 (Active Sensors)**，主动和被动的区别是主动传感器可以自主发射信号去探测环境，如激光雷达等

3D传感器生成的3D数据，可以提供更丰富的几何、形状和尺寸信息，相比于2D数据，便于计算机更好的去理解环境

## 相机

被动传感器中单目相机具有信息丰富的颜色和纹理属性、更好的路标文本视觉识别、高帧速率和可忽略不计的成本等优点，然而，它缺乏深度信息，这对于准确的位置估计至关重要。为了克服这一点，立体相机使用匹配算法来对齐左右图像中的对应关系以进行深度恢复。

## 激光雷达

主动传感器中激光雷达是一种具有透镜、激光和探测器三个基本组件的点对点发射设备，发出的光脉冲将以三维点的形式从周围环境中反射回来，形成“点云“。高稀疏性和不规则性以及缺乏纹理属性是点云的主要特征，它与图像阵列有很好的区别，激光雷达的另一个问题是部署成本高。

## 相机系统

我们想输入一张图片，然后输出一个三维形状来表示，那么我们必须使用一个坐标系，或者说我们必须使用一个**标准坐标（canonical coordinate）**，这意味着我们对每个对象类别的对象进行排序的时候，某种程度上就固定了标准方向，比如说下图中的椅子，可能Z轴正方向就是座椅前方

![79](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_80.jpg?)

另一种选择是在**视图坐标系（View Coordinates）**下进行预测，这会使得三维坐标系与输入图像对齐，是表示三维形状坐标系的另一个选择，很多人认为这是一种更容易的坐标系

标准坐标系有一个问题，那就是输出特征和输入特征不再对其，视图坐标系的话，在特征位置上，输出总是与输入相对应的，出于这一点，如果相同的网络，使用不同的坐标，那么标准网络更容易过拟合，视图网络泛化性能更好

![81](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/81.jpg?)

# 数据集

ImageNet如此成功，以至于人们创建一个数据集就想为其起名字，这也就是ShapeNet这么命名的原因，并且随着三维视觉的发展，三维数据集也逐渐建立

目前，三维形状分类的数据集有两种，合成数据集（Synthetic Dataset）和真实数据集（Real-world Dataset），前者中的物体是完整的，但是没有任何遮挡和背景，真实数据集中的物体有不同程度的遮挡，并且存在背景噪声

对于目标检测与跟踪来说，有室内数据集（Indoor Scenes）和室外城市场景（Outdoor Urban Scenes）

ShapeNet是一个三维模型数据集，提供了五十个类别的五万个模型

![87](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_88.jpg?)

但是，这个数据集是合成的，只有孤立的物体，没有上下文信息，所以需要其他的数据集，也就是Pix3D

它有家具的三维模型，或者说这是真实世界的图像，有更多的上下文信息，但是太小了

# 三维预测：Mesh R-CNN

这是R-CNN的三维版本，输入一个真实世界的RGB图像，然后为每个检测到的图像发出一个完整的三角网格，提供每个对象的三维形状

![88](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_89.jpg?)

Mesh R-CNN还是有很多优点的，比如说可以做出各种输出

![89](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_90.jpg?)

## 方法改进：新的迭代精细化

但是，网格变形的方法限制了可输出的三维形状的拓扑结构，这是因为输出必须有这与初始网格相同的拓扑结构，如果拓扑结构不同，比如说球体就无法变形为甜甜圈，这也是迭代精细化方法的一个局限性，这也是需要克服的

![91](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_92.jpg?)

克服的方法，就是首先进行粗糙的体素预测，然后将其转换为三角网格，得到这个网格之后继续迭代精细化操作，下图就展示了我们这个操作，对一个输入的RGB图像，首先进行二维目标检测，然后进行三维物体的体素重建，然后转化为三角网格进行迭代精细化，这样就可以输出有任意拓扑结构的网格了

![95](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_96.jpg?)

此外还有一些示例，都可以看到，效果还是十分精细的

![96](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_97.jpg?)

## 正则化方式

当然，这里还有一个小问题，然后我们只使用钝化距离作为损失函数进行训练，可能得到非常难以理解的结构，所以我们需要使用一些正则化方法来实现视觉上更符合人类审美的输出

![97](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_98.jpg?)

除了钝化距离以为，我们还最小化了网格中每条边的L2范数

![98](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_99.jpg?)

可以看到，模型可以输出细粒度更好的形状

当然，我们知道这个模型是建立在一个目标检测框架上的，所以可以检测非常多的对象，甚至可以预测对象的不可见部分，比如说下图中被狗头遮挡的沙发

![100](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_101.jpg?)

当然这个也有不足，就是如果二维视觉中失败的地方，往往也会成为三维视觉中失败的地方，比如说那些被遮挡的物体

![101](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L17_102.jpg?)

