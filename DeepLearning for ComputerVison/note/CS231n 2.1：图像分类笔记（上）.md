# 图像分类

## 概念

**目标**：这一节我们将介绍图像分类问题。所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像，或者简单的说，就是对于一个给定的图像，预测它属于哪个类别（或者给出属于一系列不同标签的可能性）。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。在这里我们面临的挑战就是**语义鸿沟**。在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。

**例子**：以下图为例，图像分类模型读取该图片，并生成该图片属于集合 {cat, dog, hat, mug}中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的3维数组（在深度学习工具中，图像就是一个三维张量）。在下图这个例子中，猫的图像大小是宽248像素，高400像素，有3个颜色通道，分别是红、绿和蓝（简称RGB）。

如此，该图像就包含了248X400X3=297600个数字，每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如"猫"。或者说，我们需要借助某种方法将这个原始数字网络转变为相应的有意义的语义——比如说“猫”标签

![][12]

## 困难和挑战

对于人来说，识别出一个像"猫"一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难

* **视角变化（**Viewpoint variation**）**：同一个物体，摄像机可以从多个角度来展现，尽管可能角度的变化很轻微，但是可能使得这些数字发生不直观的某种改变
* **类内差异（**Intra-class variation**）**：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。比如说猫就是一种很会变形的生物
* **相似类（**Fine-Grained Categories**）**：不同类物体的个体之间的外形差异小
* **背景干扰（**Background clutter**）**：物体可能混入背景之中，使之难以被辨认
* **光照条件（**Illumination conditions**）**：在像素层面上，光照的影响非常大，比如说光照和昏暗情况下图像会有不同情况
* **形变（**Deformation**）**：很多东西的形状并非一成不变，会有很大变化。
* **遮挡（**Occlusion**）**：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的，比如说猫隐藏在草丛中，并不明显
* **大小变化（**Scale variation**）**：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。

面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。  

![][13]

当然，图像分类有不同的境界

![YSAI_ImageClassification_L1_7](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_7.png)

通用的多类别分类，是在不同物种的层次上进行分类，比如说我们只需要区分出猫和狗即可

子类细粒度分类是对一个大类中的子类进行划分，比如说我们区分了猫狗，我们需要进一步区别是什么品种的猫和狗

最高的是实例级分类，一个个体就是一个类，这种的细粒度最高

### 类别不平衡问题

如果不同类别下样本数量相差很大，就会导致分类模型的性能变差，就与学生极度偏科一样，总分无法特别高，我们可以使用不同方面的方法提供解决方案

![YSAI_ImageClassification_L2_24](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L2_24.png?raw=true)

我们可以将样本数量很少的类别，随机进行一些插值，比如说随机选择一些图片进行复制添加到该类别包含的图像内，实际上这就是一种插值方式

![YSAI_ImageClassification_L2_25](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L2_25.png)

当然我们可以以结果为导向进行提高，删除一些分类好的类别的样本，复制一些比较差的样本的类别

![YSAI_ImageClassification_L2_27](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L2_27.png)

或者我们可以对算法本身进行改进，先构建一个比较均衡的数据集，然后在这个上面进行预训练，后面进行微调，也可以有一定程度上的提高

![YSAI_ImageClassification_L2_28](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L2_28.png)

也可以进行基于样本数量的自适应加权，提高数量少的类别的权重

研究类别数量不均衡的问题是有意义的，因为在一些特定样本难以获取的任务上，样本数量几乎是必定极为不均衡的，比如说医疗视觉模型任务中，一些罕见病的影像数量很少

当然，我们也可以尝试通过迁移学习解决，如果预训练模型的训练数据足够大，并且与本任务相匹配，那么这个预训练模型所学的特征就会具有一定的通用性，比如说用途广泛的ImageNet数据集

当然我们也可以使用数据增强方式，人为扩充数据集，这样子就可以一定程度上解决数据集的问题

![YSAI_ImageClassification_L2_32](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L2_32.png)


## 目标检测任务

当然，计算机视觉不止有图像分类，还有另一个相关的任务——目标检测，这个任务我们需要将图像中的目标对象圈出来

结果证明，图像分类本来就是一个基础，可以用来构建更多更复杂的应用程序，比如说目标检测等
![](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/object_detection.jpg)
—————————————————————————————————————————

**想法**：如何实现图像分类？

根据之前所了解到的方法，我们可能首先想到通过对照片进行**边缘检测**来提取特征，如何尝试找到角点或者其他类型的可解释模式，比如说猫有三角形尖耳朵，所以可以通过检测这方面的边缘信息，或者我们知道猫有胡须，所以我们可以提取胡须的边缘信息，我们根据这些信息来写一个算法来检测他们

当然，这并不是一个很好的方法，比如说会有没有胡须的猫，会有没有尖耳朵的猫，或者有时候边缘检测器会失效从而无法正常提交所需的边缘，而且这很难进行迁移——当我们可以成功识别猫的时候，如果我们想将其用到其他方面，比如说识别狗，那么之前的工作将毫无意义，所以我们需要找到一种具有可扩展性的算法

## 数据驱动方法

最开始的方法并不是这种数据驱动（或者深度学习）的方法，而是由专家去手动设计特征去提取，而非让机器自动去学习如何提取特征和哪些提取特征

![YSAI_ImageClassification_L1_12](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_12.png)

如何写一个图像分类的算法呢？这和写个排序算法可是大不一样。怎么写一个从图像中认出猫的算法？搞不清楚。因此，与其在代码中直接写明各类物体到底看起来是什么样的，倒不如说我们采取的方法和教小孩儿看图识物类似：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是_数据驱动方法_。也就是使用拥有从数据中学习如何识别不同类型对象与图像的算法。既然该方法的第一步就是收集大量已经做好分类标注的图片来作为训练集，那么下面就看看数据集到底长什么样：  

—————————————————————————————————————————

![][14]这是一个有4个视觉类别的训练集，尽管这个数据集非常简陋。在实际中，我们可能有上千的分类，每个分类都有成千上万的图像。



**图像分类流程**。在课程视频中已经学习过，**图像分类**就是输入一个元素为像素值的数组，然后给它分配一个分类标签。完整流程如下：

* **输入**：输入是包含N个图像的集合，每个图像的标签是K种分类标签中的一种。这个集合称为训练集。
* **学习**：这一步的任务是使用训练集来学习每个类到底长什么样。一般该步骤叫做**训练分类器**或者学习一个模型。
* **评价**：让分类器来预测它未曾见过的图像的分类标签，并以此来评价分类器的质量。我们会把分类器预测的标签和图像真正的分类标签对比。毫无疑问，如果分类器预测的分类标签和图像真正的分类标签一致，那就是好事，这样的情况越多越好。

## 数据集

### MNIST数据集：计算机视觉中的果蝇

MNIST数据集是一种手写数字数据集，其中的每张图片都包括一个不同的手写数字，大小统一为28 $\times$ 28，十个类别，有五万张作为训练集和一万张作为测试集

它更像一种玩具数据集，或者也被称为计算机视觉的果蝇，可以做很多测试，因为这个数据集很小而且简单，可以快速验证新想法

### CIFAR数据集

**CIFAR-10**：一个非常流行的图像分类数据集是[CIFAR-10][15]。这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种并且只有一个主体对象。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。可以视作MNIST的彩色增强版

在下图中你可以看见10个类的10张随机图片。

![][16]**左边**：从[CIFAR-10][15]数据库来的样本图像。**右边**：第一列是测试图像，然后第一列的每个测试图像右边是使用Nearest Neighbor算法，根据像素差异，从训练集中选出的10张最类似的图片。  

此外还有CIFAR100数据集（100类），细粒度更高，有一百个类，并且每五个类组成一个超类

每个类有600个图像，并且每个图像有两个标签——一个是精细的类标签，一个是粗糙的超类标签，如下图所示

![YSAI_ImageClassification_L1_19](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_19.png)

### PASCAL数据集

这个数据集最开始是分类的，但是后面增加了检测的一系列标注

![YSAI_ImageClassification_L1_20](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_20.png)

### ImageNet：黄金数据集

这是一个非常大的数据，有两万种类别，同时有千万张图片（每个类别1300张），百万标注框，图像大小不统一，以其命名的ImageNet分类竞赛使用了其中一千个类的子集，在此竞赛中出现了大量经典工作，如AlexNet等![YSAI_ImageClassification_L1_22](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_22.png)

我们可以看到，一个图片的标注可以分为很多个级别，颗粒度越来越小（或者说一个图片有多个标签并且呈包含关系）

![YSAI_ImageClassification_L1_24](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_24.png)

## 评价指标

### 正负样本

指标首先是统计正负样本

1. **正样本（Positive Samples）**：
   - 正样本是指那些标签或输出是正类的数据点。在二元分类中，“正”通常意味着我们感兴趣的类别，或者是我们希望模型检测或预测的类别。
   - 例如，在垃圾邮件检测问题中，垃圾邮件是正样本，因为模型的任务是识别出垃圾邮件。
2. **负样本（Negative Samples）**：
   - 负样本是指那些标签或输出是负类的数据点。在二元分类中，“负”通常意味着非目标类别，即我们不希望模型检测或预测的类别。
   - 在垃圾邮件检测的例子中，非垃圾邮件是负样本。

![YSAI_ImageClassification_L1_27](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_27.png)

### PR曲线

1. Precision：也叫查准率，表示所有被模型判定为正样本中，真正的正样本的比例。意思是检查正确样本是否准确
2. Recall：也叫查全率或者召回率，表示所有真正的正样本中，被模型判定为正样本的比例。意思是正确样本是否查找全了

![AutoDriveHeart_YOLO_L1_28](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/AutoDriveHeart_YOLO_L1_28.png)

通常会为每个物体类别绘制一个PR曲线（Precision-Recall曲线），然后计算该曲线下的面积，即AP（Average Precision，平均精度）。AP值越高，说明模型在该类别的检测性能越好。

不过精度和召回率是相互矛盾的，召回率越高，模型越倾向于把更多样本归类为正样本，就容易误判

所以我们需要综合考虑，也就是PR曲线的面积越大，性能就越好，但是PR曲线对正负样本不均衡很敏感，PR曲线更关注正样本的预测准确性，因此在正样本较少的情况下，模型性能的微小变化也会在PR曲线上表现得非常明显。

![YSAI_ImageClassification_L1_29](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_29.png)

所以我们也会使用ROC曲线进行更全面的判断

### ROC曲线与AUC

![YSAI_ImageClassification_L1_30](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_30.png)

AUC就是ROC曲线下的面积，AUC曲线对正负样本不平衡的敏感度较低。即使在负样本数量远超正样本的情况下，AUC仍能提供较为合理的性能评估。这是因为AUC考虑了所有可能的分类阈值，并且FPR的计算抵消了负样本数量多的影响。

### 混淆矩阵

对于多分类问题，比如说有K类，就有一个KxK的混淆矩阵，元素$c_{ij}$表示第i类样本被分类器判定为第j类的数量，主对角线是正确分类的样本数量，我们可以通过观察混淆矩阵，判断哪些类更容易混淆

![YSAI_ImageClassification_L1_33](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/YSAI_ImageClassification_L1_33.png)

## Nearest Neighbor分类器

作为课程介绍的第一个方法，我们来实现一个**Nearest Neighbor分类器**。虽然这个分类器和卷积神经网络没有任何关系，实际中也极少使用而且其非常简单，但通过实现它，可以让读者对于解决图像分类问题的方法有个基本的认识，也就是机器学习系统的两个基本部分——训练、预测。

其中，训练函数，就是记住所有的数据和标签（或者说进行学习），预测函数，就是预测出图像最可能的标签

![][16]

假设现在我们有CIFAR-10的50000张图片（每种分类5000张）作为训练集，我们希望将余下的10000作为测试集并给他们打上标签。Nearest Neighbor算法将会拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。上面右边的图片就展示了这样的结果。请注意上面10个分类中，只有3个是准确的。比如第8行中，马头被分类为一个红色的跑车，原因在于红色跑车的黑色背景非常强烈，所以这匹马就被错误分类为跑车了。

那么具体如何比较两张图片的相似程度呢（或者可以将相似程度理解为距离，距离越近，图片越相似）？在本例中，就是比较32x32x3的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量![I_1][17]和![I_2][18]，然后计算他们的**L1距离（曼哈顿距离）：**

![EECS498_L2_61](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L2_61.jpg)

这里的求和是针对所有的像素。下面是整个比较流程的图例：  

![][20]以图片中的一个颜色通道为例来进行说明。两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。

—————————————————————————————————————————

下面，让我们看看如何用代码来实现这个分类器。首先，我们将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，**Xtr**（大小是50000x32x32x3）存有训练集中所有的图像，**Ytr**是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：
    
```python
Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magifunction we provide
# flatten out all images to be one-dimensional
Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rowbecomes 50000 x 3072
Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rowbecomes 10000 x 3072
```

现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：  


```python
    nn = NearestNeighbor() # create a Nearest Neighbor classifier class
​    nn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels
​    Yte_predict = nn.predict(Xte_rows) # predict labels on the test images
​    # and now print the classification accuracy, which is the average number
​    # of examples that are correctly predicted (i.e. label matches)
​    print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )
```

作为评价标准，我们常常使用**准确率**，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：**train(X, y)**函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个**predict(X)**函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：  


```python
    import numpy as np
​    
​    class NearestNeighbor(object):
​      def __init__(self):
​        pass
​    
      def train(self, X, y):
        """ X is N x D where each row is an example. Y is 1-dimension of size N """
        # the nearest neighbor classifier simply remembers all the training data
        self.Xtr = X
        self.ytr = y
    
      def predict(self, X):
        """ X is N x D where each row is an example we wish to predict label for """
        num_test = X.shape[0]
        # lets make sure that the output type matches the input type
        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)
    
        # loop over all test rows
        for i in xrange(num_test):
          # find the nearest training image to the i'th test image
          # using the L1 distance (sum of absolute value differences)
          distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
          min_index = np.argmin(distances) # get the index with smallest distance
          Ypred[i] = self.ytr[min_index] # predict the label of the nearest example
    
        return Ypred
```


如果你用这段代码跑CIFAR-10，你会发现准确率能达到**38.6%**。这比随机猜测的10%要好，但是比人类识别的水平（[据研究推测是94%__][21]）和卷积神经网络能达到的95%还是差多了。点击查看基于CIFAR-10数据的[Kaggle算法竞赛排行榜__][22]。

**距离选择**：计算向量间的距离有很多种方法，另一个常用的方法是**L2距离**，从几何学的角度，可以理解为它在计算两个向量间的欧式距离。L2距离的公式如下：  

![EECS498_L2_61](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L2_61.jpg)

换句话说，我们依旧是在计算像素间的差值，只是先求其平方，然后把这些平方全部加起来，最后对这个和开方。在Numpy中，我们只需要替换上面代码中的1行代码就行：  


```python
    distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))
```

注意在这里使用了**np.sqrt**，但是在实际中可能不用。因为求平方根函数是一个单调函数，它对不同距离的绝对值求平方根虽然改变了数值大小，但依然保持了不同距离大小的顺序。所以用不用它，都能够对像素差异的大小进行正确比较。如果你在CIFAR-10上面跑这个模型，正确率是**35.4%**，比刚才低了一点。

**L1和L2比较**。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在[p-norm__](https://planetmath.org/vectorpnorm)常用的特殊形式。

![EECS498_L2_61](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L2_61.jpg)

## k-Nearest Neighbor分类器

你可能注意到了，为什么只用最相似的1张图片的标签来作为测试图像的标签呢？这不是很奇怪吗！是的，使用**k-Nearest Neighbor分类器**就能做得更好。它的思想很简单：与其只找最相近的那1个图片的标签，我们找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。  

—————————————————————————————————————————


![51aef845faa10195e33bdd4657592f86_r](https://github.com/Michael-Jetson/ML_DL_CV_with_pytorch/assets/114546283/bdd1fa05-cb3c-4dc5-b24f-940987cdb225)上面示例展示了Nearest Neighbor分类器和5-Nearest Neighbor分类器的区别。例子使用了2维的点来表示，分成3类（红、蓝和绿）。不同颜色区域代表的是使用L2距离的分类器的**决策边界**。白色的区域是分类模糊的例子（即图像与两个以上的分类标签绑定）。需要注意的是，在NN分类器中，异常的数据点（比如：在蓝色区域中的绿点）制造出一个不正确预测的孤岛。5-NN分类器将这些不规则都平滑了，使得它针对测试数据的**泛化（generalization）**能力更好（例子中未展示）。注意，5-NN中也存在一些灰色区域，这些区域是因为近邻标签的最高票数相同导致的（比如：2个邻居是红色，2个邻居是蓝色，还有1个是绿色)。

—————————————————————————————————————————

在实际中，大多使用k-NN分类器。但是k值如何确定呢？接下来就讨论这个问题。  

**图像分类笔记（上）完。**

点击查看[图像分类笔记（下）][26]。

## **译者反馈**：

1. 因CS231n的单篇课程笔记长度长，完整阅读需较长时间。知友[张欣][11]等建议对其进行适当分拆。故本次翻译将[图像分类笔记__][7]拆分为上下篇，每篇阅读量控制在1万字左右，**降低阅读成本**。效果如何，请知友们点评；
2. 知乎专栏的公式编辑器可以使用LaTeX语法，赞。但**公式居中需手动空格，有无更优雅的方式**？请知友们评论指教；
3. 翻译中的任何不足之处，请知友们在评论中批评指正，我们会及时讨论并作出反馈；
4. 本翻译是无偿行为，知乎专栏首发。允许转载，请全文保留并注明出处。

「真诚赞赏，手留余香」

机器学习计算机视觉斯坦福大学 (Stanford University)

[ShiqingFan](9)

[猴子](33)亦有帮助

![cs kite][27]

![5plus21][34]

![史书欢][27]

![eric][35]

![袁野][36]

[1]: https://pic4.zhimg.com/4a97d93d652f45ededf2ebab9a13f22b_m.jpeg
[2]: https://zhuanlan.zhihu.com/intelligentunit
[3]: https://zhuanlan.zhihu.com/write
[4]: https://pic3.zhimg.com/4b149e4f05ca3551cadcb07c0963cb6a_r.png
[5]: https://pic2.zhimg.com/5ab5b93bd_xs.jpg
[6]: https://www.zhihu.com/people/du-ke
[7]: http://link.zhihu.com/?target=http%3A//cs231n.github.io/classification
[8]: http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/
[9]: https://www.zhihu.com/people/sqfan
[10]: https://www.zhihu.com/people/gong-zi-jia-57
[11]: https://www.zhihu.com/people/zhangxinnan
[12]: https://pic2.zhimg.com/baab9e4b97aceb77ec70abeda6be022d_b.png
[13]: https://pic2.zhimg.com/1ee9457872f773d671dd5b225647ef45_b.jpg
[14]: https://pic1.zhimg.com/bbbfd2e6878d6f5d2a82f8239addbbc0_b.jpg
[15]: http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ekriz/cifar.html
[16]: https://pic1.zhimg.com/fff49fd8cec00f77f657a4c4a679b030_b.jpg
[17]: http://zhihu.com/equation?tex=I_1
[18]: http://zhihu.com/equation?tex=I_2
[19]: 

[20]: https://pic2.zhimg.com/95cfe7d9efb83806299c218e0710a6c5_b.jpg
[21]: http://link.zhihu.com/?target=http%3A//karpathy.github.io/2011/04/27/manually-classifying-cifar10/
[22]: http://link.zhihu.com/?target=http%3A//www.kaggle.com/c/cifar-10/leaderboard
[23]: http://zhihu.com/equation?tex=%5Cdisplaystyle+d_2%28I_1%2CI_2%29%3D%5Csqrt%7B+%5Csum_p%28I%5Ep_1-I%5Ep_2%29%5E2%7D
[24]: http://link.zhihu.com/?target=http%3A//planetmath.org/vectorpnorm
[25]: https://pic3.zhimg.com/51aef845faa10195e33bdd4657592f86_b.jpg
[26]: https://zhuanlan.zhihu.com/p/20900216

  
