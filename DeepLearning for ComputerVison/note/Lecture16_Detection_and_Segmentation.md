# 回顾目标检测

## 历史发展

计算机视觉的历史可以追溯到很久之前

在深度学习元年（2012年）之前，所有的计算机视觉任务的精度都不是很高，发展平稳，而且在发展到一段时间后陷入了瓶颈期（2010-2012的mAP指数上升缓慢）

![EECS498_L16_5](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_5.jpg)

但是在2013年开始，人们将深度学习应用于计算机视觉任务中，使得视觉的精度有了快速的增长，而且还是持续的增长

当然上面的测试是在VOC数据集上测试的，这是一种相对简单的数据集，所以在2016年开始，人们转而去处理更具挑战性的数据集，大多数方法甚至不再费心测试这个数据集

## 回顾目标检测任务

在第一个经典目标检测算法R-CNN中，使用训练好的CNN来对那些使用选择性搜索方法筛选出来的区域进行预测，然后尝试拟合出一个最好的边界框，因为最开始的方法是基于选择性搜索的，所以这种候选区域方法可以视为黑盒

![EECS498_L16_6](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_6.jpg)

然后，我们使用先卷积提取特征然后提取候选区域，这种方法大大提高了目标检测的效率；后来，神经网络方法替代了选择性搜索，目标检测速度也再度提高

## R-CNN系列模型的训练方法

在上一节里面，老师并没有讲到R-CNN应该如何完成训练，所以这里对训练方法进行讲解

### Slow R-CNN

在Slow R-CNN（第一代R-CNN的戏称）中，先接受一个彩色图像，并且可以访问到其中的真实边界框和标签，然后在输入图像上运行候选区域法，筛选出来一些候选区域

不过注意一下，选择性搜索是一个固定的算法，而非学习的部分。

选择性搜索是一种基于贪心策略的区域提议算法。它首先在多种尺度和颜色空间下对图像进行分割，然后根据颜色、纹理、大小和形状兼容性来合并相似的区域，生成一系列的候选区域。这个过程并不涉及学习，也就是说，它不会根据训练数据来调整自己的行为。

然后，在训练的时候，还有一个重要步骤，就是使用第一个CNN对候选区域进行分类，让其区分背景和对象，可以通过与真实边界框与标签进行对比来完成这一点

如下图所示，GT边界框表示真实边界框，包括两只狗和一只猫，其他颜色的是所有候选区域，可以看到，有的候选区域与GT边界框很吻合（深蓝色边界框），有的只包括对象的一部分（浅蓝色边界框），有的则只是背景（红色边界框）

![EECS498_L16_10](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_10.jpg)

我们知道在Slow R-CNN中我们会使用一个卷积网络对候选区域进行分类，分为正的目标类和负的背景类，当然，在这里我们还有一种类，就如上图中的浅蓝色框，只包含了对象的一部分，虽然也不是错误的，但是离完整的边界框还有一段距离，可以称为中性框

至于如何区分，可以使用交并比来判断，比如说一个区域与正边界框的交并比小于0.3，就可以认为这是负边界框，这里的交并比阈值也是一个超参数

这样，我们就得到了一些符合实际情况的边界框，比如说绝对不包括对象的负边界框，基本包括对象的正边界框还有位于中间状态的中性框

但是我们在训练的时候会忽略中性框，因为在这上面训练网络的话可能会产生混淆，中性框不是我们需要的也不是我们想排除的，所以在训练的时候直接忽视

下一步，挑选出来正边界框或者正类候选区域，将裁剪出的图像进行大小调整，转化为标准分辨率的图像，然后使用另一个卷积神经网络去预测类别和边界框回归变换

![EECS498_L16_12](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_12.jpg)

也就是说，对于负类，我们只需要区分类别就可以，对于正类，我们再进行分类和定位，所以需要让二分类网络记住这些背景区域和目标区域

### Fast R-CNN

Fast R-CNN的训练方法与Slow R-CNN的方法基本一致，只不过因为在Fast R-CNN中是先卷积来提取特征，然后使用候选区域法裁剪特征，这样就减少了大量重复计算，加快了效率

## 特征裁剪

不过在Fast R-CNN中，会使用一种特征裁剪的方式RoI池化，来对区域进行投影，投影到特征图上，这样就可以简化计算，然后对区域对应的特征图进行池化，然后输入到第二阶段中去，这样可以便于对齐，可以更好的进行目标检测

![EECS498_L16_21](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_21.jpg)

我们实现的方法就是使用双线性插值，在图像的特征图的单元直接进行插值

![EECS498_L16_24](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_24.jpg)

这种方式还可以解决反向传播问题

## 不依赖锚框的目标检测方法

我们知道，Faster R-CNN和单阶段目标检测方法是依赖于锚框的，所以有没有不依赖于锚框的检测方法呢

![EECS498_L16_31](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_31.jpg)

实际上是有的，那就是密歇根大学的几个教授在2018年完成的这篇论文，其中的创新点是使用CornerNet来实现目标检测，不同于之前的方法

![EECS498_L16_32](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_32.jpg)

这个网络的想法是，改变我们对边界框的参数化方式，我们现在要用左上角和右下角来表示边界框，然后用来检测边界框，我们只需要让让图像的像素决定

1. **无需候选框生成**：Faster R-CNN 使用了区域提议网络（Region Proposal Network，RPN）来生成候选框。相比之下，CornerNet 直接在图像中预测目标的两个角（左上角和右下角），无需生成和处理大量的候选框。这种方法避免了候选框生成的计算复杂性和可能的错误，提高了效率。
2. **基于关键点的检测方式**：CornerNet 使用的是一种基于关键点的目标检测方法，它预测的是目标的角点，而不是常见的边界框。这种方式在理论上可以更精确地定位目标，因为角点的位置可以是任意的，而不仅仅是像边界框那样的轴对齐的矩形。
3. **单阶段检测**：CornerNet 是一种单阶段的目标检测方法，即它直接在输入图像上预测目标的位置和类别，无需像 Faster R-CNN 那样分成两个阶段进行。这也使得 CornerNet 在速度上具有优势。
4. **损失函数的改进**：CornerNet 使用了一种新的损失函数，即 focal loss 和 corner pooling，这在一定程度上解决了类别不平衡问题，并改进了角点的预测性能。

# 语义分割（Semantic Segmentation）

语义分割任务就是我们对图像中的每个像素打标签，比如说下图中这只猫，我们希望对属于猫这个对象的每个像素都打上相应的标签

![EECS498_L16_35](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_35.jpg)

当然，在语义分割中，并不会区分不同的对象，只是负责给对象打类别标签，也就是说，如果有两个紧邻的同一个类别的对象，那么会打上同一种颜色的标签（如上图中的奶牛），不会区分不同的实例

## 初步：滑动窗口法

我们有一个不成熟的想法，就是使用滑动窗口的方法，不断预测某一个小区域的类别，然后给予相应的标签

![EECS498_L16_37](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_37.jpg)

当然这种方法效率是很低的，因为需要大量重复的计算，并且没有共享特征

实际上我们绝对不会在实践中使用这种方法，但是我们可以从中获得一些启发

## 全连接卷积网络

我们使用一种名为**全连接卷积网络（Fully Convolutional Network）**的网络架构来实现语义分割任务，这是一个没有全连接层和全局池化层的网络，它只有一堆卷积层，输出与输入有同样的二维形状（或者说输出形状由输入形状决定），输出是每个图像的分类分数，或者说具有与类别数一样的通道数的三维张量，然后使用最大化函数就可以得到一个我们想要的语义分割图

然后损失函数则是每个像素的交叉熵损失

![EECS498_L16_38](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_38.jpg)

当然，模型如何知道图像中有多少类别呢？其实这个类似于分类网络，我们预先选择一组类别（这个由训练数据集决定），然后进行设置

为了做出好的分割决策，我们实际上可能希望根据输入图像中相对较大的区域来做出决定，所以我们想，如果使用3x3卷积，那么感受野大小会是线性增长的，两个3x3卷积叠在一起，那么第二层的输出实际上就是查看5x5的区域，每多一层，感受野大小就会加二，这意味着我们实际上需要非常多的层才能获得非常大的感受野大小

![EECS498_L16_40](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_40.jpg)

所以就会产生问题，我们经常想要在相对高分辨率的图像上完成分割，比如说互联网图像和卫星图像，这些图像可能有以百万计的像素，导致这种方式的计算量非常大，所以需要进行改进

实际上，人们会使用上采样和下采样的架构来完成语义分割

## 带有上/下采样的全连接卷积网络

首先解释一下什么是上采样和下采样

**下采样（Downsampling）**：下采样是一种降低数据的分辨率或者复杂度的处理方式。在图像处理中，下采样通常是指减少图像的像素数量，使图像变得更小、更模糊。在深度学习中，下采样通常通过使用卷积层（特别是步长大于1的卷积层）或池化层（如最大池化或平均池化）来实现。通过下采样，我们可以减少计算的复杂度，同时提取出图像的高层次（也就是更抽象的）特征。然而，下采样的过程会丢失一些细节信息，这对于需要精确像素级别预测的语义分割任务来说，可能是一个问题。

**上采样（Upsampling）**：上采样是一种增加数据分辨率或者复杂度的处理方式。在图像处理中，上采样通常指增加图像的像素数量，使图像变得更大、更清晰。在深度学习中，上采样通常通过使用转置卷积（也叫反卷积）或者插值（如最近邻插值或双线性插值）来实现。上采样的目标是恢复下采样过程中丢失的细节信息，从而使我们可以在高分辨率的输出上进行预测。这对于语义分割任务来说，是非常重要的。

在网络开始的时候进行下采样，这样就可以提取很多特征，就类似于图像分类中一样，我们甚至可以使用池化方式去进行下采样，但是什么是上采样呢

![EECS498_L16_42](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_42.jpg)

上采样可能是一种与池化操作相反的过程，或者可以称为**反池化（Unpooling）**

比如说我们输入一个2x2大小的矩阵，上采样就可以实现一个空间上两倍大小的输出，通道数不变

![EECS498_L16_44](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_44.jpg)

**钉床（Bed of Nails，也可译为床钉）**：这种方法的名字来源于它的工作原理与形状。"Bed of Nails"是一种上采样策略，其工作原理类似于在下采样时丢失的位置放置"钉子"（即非零值），然后通过插值或其他方法在其他位置生成值，从而恢复原始的分辨率。在这里，我们是将原特征向量复制到每个对应区域的左上角，然后其他办法用零值填充。当然这不是一个很好的方法，所以人们通常使用另一种方法。

**最近邻反池化（Nearest Neighbor）**：最近邻上采样方法的基本思想是：对于每一个在上采样后的新的像素位置，选取最近的原始像素值作为其值。因为它直接使用最近的像素值，所以这种方法相对简单快速，但可能会在图像中引入一些锯齿状的效果（因为它不会像一些其他上采样方法那样进行平滑处理）。

在实践中，最近邻上采样常常被用在一些需要快速但不需要过分关注图像质量的场景中。在深度学习的语义分割任务中，尤其是在进行分辨率恢复（即上采样操作）的时候，最近邻上采样也是一个常见的选择。

**双线性插值（Bilinear Interpolation）**：在图像上采样中，双线性插值的操作可以这样理解：对于目标图像中的每一个像素，先在水平方向进行线性插值，然后在垂直方向进行线性插值，得到最终的像素值。

![EECS498_L16_45](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_45.jpg)

举个例子，如果我们想在一个2x2的像素格子中插入一个新的像素，我们可以先在水平方向上，对左侧像素和右侧像素进行线性插值，然后在垂直方向上，对上侧像素和下侧像素进行线性插值，最后，我们就得到了新的像素的值。

相比于最近邻插值，双线性插值能够得到更加平滑的图像，因为它在插值过程中，考虑了像素之间的关系，能够更好地保留图像的细节信息。但同时，双线性插值的计算量也比最近邻插值要大一些。

**双三次插值**：它是一种更高级的插值方式，相比于最近邻插值和双线性插值，可以得到更加平滑、更少锯齿状的图像。

![EECS498_L16_46](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_46.jpg?raw=true)

在双三次插值中，新的像素值是通过考察附近16个像素（在一个4x4的区域内）并使用这些像素的值进行加权计算得到的。具体的加权系数由三次插值函数决定，这个插值函数考虑到了像素之间的距离关系。

由于双三次插值考虑了更多的邻近像素，并且使用了更为复杂的插值函数，所以它能够得到更加精细、更为平滑的图像。但是，双三次插值的计算复杂度也较高，需要更多的计算资源。

**最大反池化（Max Unpooling）**：这种方法的主要目的是恢复被池化（pooling）操作降低的图像分辨率。

![EECS498_L16_47](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_47.jpg)

在最大池化（Max Pooling）操作中，输入的特征图被分成若干个非重叠的区域，每个区域内取最大值作为该区域的代表，这样能够大大减小特征图的尺寸。但在这个过程中，我们会丢失很多细节信息，这对于语义分割任务来说是不可接受的。

为了解决这个问题，我们可以使用最大反池化操作来恢复原来的图像尺寸。具体来说，最大反池化是这样进行的：首先，创建一个和原图像尺寸相同的空矩阵，然后，将池化操作中得到的最大值填充到对应的位置上，其他位置用零填充。这样，就能够得到一个和原图像尺寸相同的特征图。

值得注意的是，最大反池化操作需要记录最大池化操作中的最大值位置，这是因为我们需要在反池化过程中将最大值放回原来的位置。因此，最大反池化操作通常与最大池化操作配合使用。

**理解**

在语义分割任务中，下采样和上采样操作通常会配合使用，以形成一个编码-解码（Encoder-Decoder）的结构。这种结构也被称为 U-Net 结构，因为它的形状类似于字母"U"。

在编码阶段（即下采样阶段），网络通过连续的卷积层和池化层来提取输入图像的语义特征。这些操作可以将图像的空间维度（即高度和宽度）降低，同时提升图像的深度（即通道数），使得网络能够捕捉到更抽象的特征。然而，这个过程会导致图像的空间信息丢失，这对于语义分割任务来说是不可接受的。

为了恢复图像的空间信息，网络在解码阶段（即上采样阶段）使用一系列的反卷积操作或上采样操作，以将特征图的空间维度恢复到与原始输入图像相同的尺寸。在这个过程中，网络通常会将上采样后的特征图与对应的下采样阶段的特征图进行拼接或相加，这被称为跳跃连接（skip connection）。这种方法可以帮助网络更好地保留低级别的空间信息，从而提高分割的精度。

经验法则是，如果你如果在下采样部分使用平均池化这种，那么上采样就可以考虑最近邻或者双线性或者双三次插值，如果下采样是最大池化，那么应该考虑使用最大反池化作为上采样方式

## 转置卷积（Transposed Convolution）：可学习的上采样

上面这些上采样运算，并没有任何可学习的参数，都是固定的运算，但是这里有一种转置卷积的上采样方法，可以通过某种方法进行学习

普通的带有步长和填充的卷积大家很熟悉了已经（如下图所示），那么可学习的卷积是什么呢？

![EECS498_L16_50](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_50.jpg)

我们知道，步长大于等于1的卷积，实际上就是一个可学习的下采样操作，去提取特征，而可学习的这个属性意味着网络可以学习如何最好地进行下采样，以便最大化某些任务的性能，这也是为什么许多现代的卷积神经网络结构，如ResNet和DenseNet，倾向于使用步长大于1的卷积替代池化操作进行下采样。

### 什么是转置卷积

但是，如果我们的步长小于1呢？这样，卷积可以通过某种方式，为输入中的每个点跨越输出中的多个点，如果我们能想出一种方法来做到这一点，那么是不是就可以实现一种可学习的上采样操作？实际上是有的，也就是**转置卷积**

![EECS498_L16_59](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_59.jpg)

它的工作方式是这样的，首先输入一个低分辨率的数据，想输出一个高分辨率的数据（如上图所示），不过与常规卷积不同，我们这里是使用输入张量中的元素乘以转置卷积核（标量乘以张量），然后得到输出，并且将数值复制到输出张量的相应位置上，然后我们在输入中移动一个位置，在输出中移动两个位置，再次执行反卷积操作，如果有重合的地方，则进行求和

或者我们可以换一个视角去看这个操作

![EECS498_L16_60](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_60.jpg)

### 矩阵化的转置卷积

在深度学习中，所有的操作都可以被表示为一种数学运算，特别是矩阵运算。我们知道，卷积操作也可以被表示为矩阵乘法。同样，转置卷积也可以被表示为矩阵乘法，但是这个矩阵是卷积矩阵的转置。

假设我们有一个卷积核，我们可以通过这个卷积核构造一个卷积矩阵。在这个卷积矩阵中，每一行对应于卷积核在输入特征图上的一个特定位置。通过将这个卷积矩阵和输入特征图进行矩阵乘法，我们可以得到卷积的结果。

当我们进行转置卷积时，我们使用的是这个卷积矩阵的转置。转置卷积矩阵中的每一行对应于卷积核在输出特征图上的一个特定位置。通过将转置卷积矩阵和输入特征图进行矩阵乘法，我们可以得到转置卷积的结果。

因此，从这个角度来看，我们可以将转置卷积理解为一种矩阵乘法，这种矩阵乘法使用的是卷积矩阵的转置。

下图是一个一维卷积的例子，左边是使用矩阵乘法代替卷积运算，右边是转置卷积，可以看到是将前面的卷积矩阵进行转置，卷积和转置卷积在数学上相同或者等价，只不过在填充等方面稍有不同

![EECS498_L16_63](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_63.jpg)

**注意一下，转置卷积不是卷积的逆运算，转置卷积也是卷积**

## 语义分割的不同任务

在计算机视觉和语义分割中，"stuff"和"thing"是两种不同类型的语义类别。

![EECS498_L16_68](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_68.jpg)

"Thing"通常指的是可数的、有独立存在的物体，如人、汽车、自行车等。这些对象通常有明确的形状和大小，而且在图像中可以被清晰地辨识出来，可以区分为不同的实例（Instance）。

相反，"Stuff"指的是不可数的、通常作为背景存在的物体，如草地、天空、路面等。这些对象通常没有明确的形状和大小，而且在图像中可能会覆盖较大的区域，这些对象讨论数量或者个体是没有意义的。

在进行语义分割时，我们需要区分这两种类型的语义类别。对于"Thing"，我们的目标通常是找出图像中的每个独立对象并给它们分配相应的类别。对于"Stuff"，我们的目标通常是识别出图像中的每个区域并给它们分配相应的类别。

在某些情况下，"Stuff"和"Thing"的区分可能会变得模糊。例如，当我们看到一群密集的鸟时，我们可能会把它们视为一整块"Stuff"，而不是单独的"Thing"。在这种情况下，如何进行语义分割就需要根据具体的任务和数据集来确定了。

# 实例分割

我们已经了解了目标检测和语义分割，前者可以区分不同对象实例，后者分割不同类别，但是抛弃了实例的概念，所以需要一种更高级的算法来实现目标检测+语义分割，也就是既要区分不同对象，也要完成像素级分割，也就是**实例分割（Instance Segmentation）**

![EECS498_L16_72](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_72.jpg)

我们想实现实例分割，就需要基于目标检测算法，对于每一个检测出的对象，输出一个**分割掩码（segmentation mask）**

## Mask R-CNN

Mask R-CNN就是一个很经典的实例分割算法

其相对于Faster R-CNN的改进就是，在从特征图上抽取出候选区域的时候，会额外附加一个分支用来完成Mask Prediction

![EECS498_L16_74](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_74.jpg)

具体工作方式是这样的，我们仍然通过骨干网络提取网络特征，然后使用RPN预测候选区域，然后通过RoI对齐来获得特征图，然后在每个候选区域上运行一个小的语义分割网络，这个网络会为检测到的对象来预测一个分割掩码

![EECS498_L16_75](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_75.jpg)

看起来，Mask R-CNN有点像目标检测和语义分割联合起来了，我们只是对检测到的对象进行语义分割，这意味着训练这些分割掩码的目标就是与候选区域的边界框对齐

如下图所示，红色边界框中是我们想检测的对象，我们的目标就是让分割掩码与检测到的对象的边界对齐

![78](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_79.jpg)

这是一个非常好的算法，他不但会使用目标检测方法完成边界框的输出，对于每个边界框，实际上也会输出分割掩码来告诉我们，边界框中的哪些像素实际上对应检测到的对象

![EECS498_L16_80](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_80.jpg)

## 全景分割（Panoptic Segmentation）

这是一种混合任务，混合了语义分割和实例分割，它想要标记每个像素，同时对于同一个类别的不同物体加以区分，或者说同时处理"stuff"类别（如天空、草地等大面积的、不可数的物体）和"thing"类别（如汽车、人等可数的、独立的物体实例）。全景分割的目标是为图像中的每个像素点都分配一个类别标签，并且在"thing"类别上区分出不同的实例。

![EECS498_L16_83](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_83.jpg)

实例分割和全景分割的主要区别在于全景分割更加全面，它同时处理了语义分割和实例分割两种任务。实例分割主要关注"thing"类别的识别和分割，而全景分割则旨在对所有的像素点进行准确的分类和分割。

# 各种应用

## 位姿估计

不过有时候，大家对于确定图像中人体的确切姿态更感兴趣，这个可以使用关键点检测的方式来完成

我们首先分割出不同的人体实例，然后使用某种方法去估计其姿势，常用的一种方法就是定义一组关键点，比如说眼睛、鼻子和各种关节

![EECS498_L16_84](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_84.jpg)

想完成这一步的一种方法，就是使用Mask R-CNN实现关键点检测，我们可以在Mask R-CNN上再加一个关键点预测网络来预测关键点

![EECS498_L16_87](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_87.jpg)

这个新加入的网络，只是预测十七个关键点，因为我们认为想确定人体姿态，十七个关键点就足够了，然后使用交叉熵损失训练

![EECS498_L16_88](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_88.jpg)

训练效果如下

![EECS498_L16_89](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_89.jpg)

## 通用想法

我们现在有了一种很通用的想法，就是如果我们想在输入图像的不同区域做某些新颖类型的预测，那么就可以将其框定为一个目标检测任务，然后在目标检测器上加一个“Heads”，这个“Heads”就可以在输入图像的每个区域产生额外的新型输出

![EECS498_L16_90](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_90.jpg)

## 密集字幕

有一个很好玩的想法，就是合并目标检测和图像字幕，输入一个图像，然后输出一种自然语言来描述输入图像中的不同区域

你可以通过将LSTM这种模型添加到特征区域顶部来实现

![EECS498_L16_92](https://raw.githubusercontent.com/Michael-Jetson/Images/main/UpGit_Auto_UpLoad/EECS498_L16_92.jpg)